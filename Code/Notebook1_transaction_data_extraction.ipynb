{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Transfer Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Extraction Notes => \n",
    "\n",
    "1. This workbook is used to extract the AAVE transfer data on ETH blockchain. \n",
    "2. The original data source is from a Kaggle live dataset called \"ethereum-blockchain\". \n",
    "3. To extract the dataset, we need to create a project on Google cloud and extract the data using bigquery. \n",
    "4. This workbook can be ran using Kaggle notebook. For the assessment purpose, I have saved each of the datasets to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra (version: 1.23.2)\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) (version: 1.5.0)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token Transfer Data Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AAVE Transfer data querying (from Google Big Query) using Kaggle's public dataset BigQuery integration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "from google.cloud import bigquery\n",
    "import numpy as np # version: 1.23.2\n",
    "import pandas as pd # version: 1.5.0\n",
    "from tqdm import tqdm # version: 4.46.1\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Create a \"Client\" object\n",
    "client = bigquery.Client(project = \"comp7860-project\")\n",
    "\n",
    "# Construct a reference to the \"crypto_ethereum\" dataset (https://www.kaggle.com/bigquery/ethereum-blockchain)\n",
    "dataset_ref = client.dataset(\"crypto_ethereum\", project=\"bigquery-public-data\")\n",
    "\n",
    "# API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "# List all the tables in the \"crypto_ethereum\" dataset\n",
    "tables = list(client.list_tables(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_csv(sql, output_path): \n",
    "    df = client.query(sql).to_dataframe(progress_bar_type='tqdm_notebook')\n",
    "    df.to_csv(output_path, mode='a', index=False, header=not os.path.exists(output_path), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT token_address, from_address, to_address,block_timestamp, cast(value AS NUMERIC) FROM \n",
    "`bigquery-public-data.crypto_ethereum.token_transfers` \n",
    "WHERE token_address = \"0x7fc66500c84a76ad7e9c93437bfc5ac33e2ddae9\"\n",
    "'''\n",
    "df = client.query(sql).to_dataframe(progress_bar_type='tqdm_notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "df.rename(columns={'f0_':'value'}, inplace = True)\n",
    "df = df.dropna()\n",
    "df['value'] = df['value'].apply(lambda x: float(x))\n",
    "df['timestamp'] = pd.to_datetime(df['block_timestamp'])\n",
    "df['timestamp'] = df['timestamp'].apply(lambda x: str(x)[:10])\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "# new data period \n",
    "df_new = df[df['timestamp']>'2021-07-10']\n",
    "df_new = df[df['timestamp']<'2022-07-09']\n",
    "# existing data period  \n",
    "df_existing = df[df['timestamp']>'2020-10-10']\n",
    "df_existing = df[df['timestamp']<'2021-10-09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the new dataset into csv\n",
    "df_new.to_csv(\"Aave_Raw_Transfer_Data_New.csv\", index=False)\n",
    "df_existing.to_csv('Aave_Raw_Transfer_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(columns = ['token_address','block_timestamp'])\n",
    "df_existing = df_existing.drop(columns = ['token_address','block_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add values between the 2 same addresses together\n",
    "df_new[['from_address', 'to_address']] = np.sort(df_new[['from_address', 'to_address']], axis=1)\n",
    "df_new = df_new.groupby(['timestamp','from_address','to_address']).agg(lambda x: sum(x)).reset_index()\n",
    "\n",
    "df_existing[['from_address', 'to_address']] = np.sort(df_existing[['from_address', 'to_address']], axis=1)\n",
    "df_existing = df_existing.groupby(['timestamp','from_address','to_address']).agg(lambda x: sum(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed datasets\n",
    "df_new.to_csv('New_Data/AAVE_transaction_data_after_preprocessing_New.csv', index=False)\n",
    "df_existing.to_csv('Existing_Data/AAVE_transaction_data_after_preprocessing.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
